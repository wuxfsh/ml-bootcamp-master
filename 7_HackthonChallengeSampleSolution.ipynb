{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML bootcamp hackthon challenge: sentiment analysis\n",
    "\n",
    "At this stage you have learned what machine learning is and you have seen examples of how to use machine learning. Now it is time to see how well YOU can generalize what you have learned to a new challenge.\n",
    "\n",
    "The hackathon has two tracks: in the **Ideation Track** you can brainstorm on what great apps we can build for the enterprise with this new technology. What are the 'jobs to be done' that machine learning can solve for us in the future? Create an Intrapreneurship-style pitch and present it to the team on Friday. \n",
    "https://jam4.sapjam.com/groups/r7ILMAl5MxS8rgHSaNR8L9/overview_page/46399\n",
    "\n",
    "In the **Data Science Track**, we will run a hackthon challenge on a popular machine learning task: predicting consumer sentiment from online reviews. You are given a set of movie reviews and their labels (positive, negative) and you have to build a system that can predict the sentiment for a new movie review.\n",
    "\n",
    "We are using the popular polarity data set from Cornell University.\n",
    "http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## download the movie review data set\n",
    "import sys, os \n",
    "import urllib.request\n",
    "\n",
    "# set http proxy env variable\n",
    "import os\n",
    "os.environ['http_proxy'] = 'proxy.sin.sap.corp:8080'\n",
    "\n",
    "# download file to /tmp\n",
    "url = \"http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\"\n",
    "file_name = '/tmp/review_polarity.tar.gz'\n",
    "if not os.path.isfile(file_name):\n",
    "    with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
    "        data = response.read() # a `bytes` object\n",
    "        out_file.write(data)\n",
    "    out_file.close()\n",
    "\n",
    "# make file world readable\n",
    "os.chmod(file_name, 0o755)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract file to /tmp dir\n",
    "import tarfile\n",
    "import os.path\n",
    "\n",
    "\n",
    "if not os.path.isdir('/tmp/txt_sentoken'):\n",
    "    os.chdir('/tmp')\n",
    "    tar = tarfile.open(file_name, \"r:gz\")\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "    \n",
    "\n",
    "# make files world readable\n",
    "for label in ['pos', 'neg']:\n",
    "    os.chmod('/tmp/txt_sentoken/%s/' % label, 0o755)\n",
    "    for file_name in os.listdir('/tmp/txt_sentoken/%s/' % label):\n",
    "        os.chmod(os.path.join('/tmp/txt_sentoken/%s/' % label, file_name), 0o755)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load positive and negative movie review text\n",
    "\n",
    "# list to store the examples\n",
    "reviews = []\n",
    "\n",
    "# load positive reviews\n",
    "for file_name in os.listdir('/tmp/txt_sentoken/pos/'):\n",
    "    with open(os.path.join('/tmp/txt_sentoken/pos/', file_name)) as fin:\n",
    "        # append sentiment 'pos' label and review text\n",
    "        reviews.append(('pos', fin.read()))\n",
    "        \n",
    "# load negative reviews\n",
    "for file_name in os.listdir('/tmp/txt_sentoken/neg/'):\n",
    "    with open(os.path.join('/tmp/txt_sentoken/neg/', file_name)) as fin:\n",
    "        # append sentiment 'neg' label and review text\n",
    "        reviews.append(('neg', fin.read()))\n",
    "\n",
    "# great, there are 2000 reviews in the data set now  \n",
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# randomly split data into 80% training and 20% testing\n",
    "# each example is a tuple consisting of a (pos/neg)  sentiment label \n",
    "# and the text of the review\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(reviews)\n",
    "\n",
    "size_train = int(len(reviews)*0.8)\n",
    "reviews_train = reviews[:size_train]\n",
    "reviews_test = reviews[size_train:]\n",
    "\n",
    "print(\"size of training set:\", len(reviews_train))\n",
    "print(\"size of test set:\", len(reviews_test))\n",
    "\n",
    "# split data and text\n",
    "labels_train = [ item[0] for item in reviews_train ]\n",
    "corpus_train = [ item[1] for item in reviews_train]\n",
    "\n",
    "labels_test = [ item[0] for item in reviews_test ]\n",
    "corpus_test = [ item[1] for item in reviews_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's look at a few examples of the training set\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=3)\n",
    "pp.pprint(reviews_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1: data rookie\n",
    "Explore the data and find out which words are good features for positive and negative reviews. Visualize the data. Remember what you have learned in the text mining notebook about pre-processing for text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## pre-process, visualize and exploree data\n",
    "\n",
    "# tokenize the text\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "corpus_train = [ ' '.join(word_tokenize(doc)) for doc in corpus_train ]\n",
    "\n",
    "# let's check it out\n",
    "pp.pprint(labels_train[:3])\n",
    "pp.pprint(corpus_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## clean corpus: remove stop words and lowercase\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    # remove stop words and not alphabetic tokens\n",
    "    return ' '.join((word for word in doc.split() if word.isalpha() and not word in stop))\n",
    "\n",
    "corpus_train_clean = [ remove_stopwords(doc).lower() for doc in corpus_train ] \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## count word tokens using python Counter\n",
    "from collections import Counter\n",
    "\n",
    "# filter out English stop words\n",
    "cnt = Counter()\n",
    "cnt_pos = Counter()\n",
    "cnt_neg = Counter()\n",
    "\n",
    "for label, doc in zip(labels_train, corpus_train_clean):\n",
    "    cnt.update(doc.split())\n",
    "    if label == 'pos':\n",
    "        cnt_pos.update(doc.split())\n",
    "    elif label == 'neg':\n",
    "        cnt_neg.update(doc.split())\n",
    "    else:\n",
    "        # should not happen\n",
    "        print(\"Warning: unknown label :\",  label)\n",
    "        assert(False)\n",
    "        \n",
    "    \n",
    "# print the 10 most common words\n",
    "print(\"Most common words in the corpus\")\n",
    "print(cnt.most_common(10))\n",
    "\n",
    "# print the 10 most common words in the positive reviews\n",
    "print(\"Most common words in the positive reviews\")\n",
    "print(cnt_pos.most_common(10))\n",
    "\n",
    "\n",
    "# print the 10 most common words in the negative reviews\n",
    "print(\"Most common words in the negative reviews\")\n",
    "print(cnt_neg.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2: data sophomore\n",
    "Build a first classifier that can predict if a given movie review is positive or negative. \n",
    "Remember what you have learned in the data science and text mining notebooks about feature extraction pipelines, training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## feature extraction, model training on the training set and evaluation on the test set using accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create numpy array for labels\n",
    "import numpy as np\n",
    "\n",
    "y_train = np.array(list(map(lambda x: +1 if x == 'pos' else -1, labels_train)))\n",
    "y_test = np.array(list(map(lambda x: +1 if x == 'pos' else -1, labels_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## baseline: random guess classifier\n",
    "\n",
    "def random_guess():\n",
    "    if random.random() > 0.5:\n",
    "        return +1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# make a random guess \n",
    "predicted = [ random_guess() for doc in corpus_test ]\n",
    "\n",
    "# evalute on the test set\n",
    "acc = np.mean(predicted == y_test)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## naive classifier that predicts +1 if the word 'good' appears in the review\n",
    "\n",
    "def naive_guess(doc):\n",
    "    if 'good' in doc.split():\n",
    "        return +1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# make a naive guess \n",
    "predicted = [ naive_guess(doc) for doc in corpus_test ]\n",
    "\n",
    "# evalute on the test set\n",
    "acc = np.mean(predicted == y_test)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## build 'vanilla' classifier pipeline with all defaults\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf', SGDClassifier(random_state=42))\n",
    "                    ])\n",
    "\n",
    "# train the model using the pipeline\n",
    "pipeline.fit(corpus_train, y_train)\n",
    "\n",
    "# evalute on the test set\n",
    "predicted = pipeline.predict(corpus_test)\n",
    "acc = np.mean(predicted == y_test)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 3: data ninja\n",
    "Improve your classifier, try out different pre-processing steps, features, classifier models, etc. \n",
    "Let's see who can build the most accurate classifier!!\n",
    "Let's the games begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## build pipelines to test different pre-processing, feaature extraction, and machine learning models\n",
    "## try to get the best accuracy on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## use tf-idf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(random_state=42))\n",
    "                    ])\n",
    "\n",
    "# train the model using the pipeline\n",
    "pipeline.fit(corpus_train, y_train)\n",
    "\n",
    "# evalute on the test set\n",
    "predicted = pipeline.predict(corpus_test)\n",
    "acc = np.mean(predicted == y_test)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## use unigrams and bigrams\n",
    "pipeline = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                     ('clf', SGDClassifier(random_state=42))\n",
    "                    ])\n",
    "\n",
    "# train the model using the pipeline\n",
    "pipeline.fit(corpus_train, y_train)\n",
    "\n",
    "# evalute on the test set\n",
    "predicted = pipeline.predict(corpus_test)\n",
    "acc = np.mean(predicted == y_test)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## do grid search over parameter space\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False)\n",
    "             }\n",
    "\n",
    "pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(random_state=42))\n",
    "                    ])\n",
    "\n",
    "# do grid search\n",
    "gs_clf = GridSearchCV(pipeline, parameters, n_jobs=1)\n",
    "\n",
    "# train the model using the pipeline\n",
    "fs_clf = gs_clf.fit(corpus_train, y_train)\n",
    "\n",
    "best_parameters, score, _ = max(gs_clf.grid_scores_, key=lambda x: x[1])\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "\n",
    "# evalute on the test set\n",
    "predicted = gs_clf.predict(corpus_test)\n",
    "acc = np.mean(predicted == y_test)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## try out different classifiers\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "names = [\"SGDClassifier\", \"Linear SVM\", \"AdaBoost\", \"NaiveBayes\"]\n",
    "\n",
    "classifiers = [\n",
    "    SGDClassifier(),\n",
    "    SVC(kernel=\"linear\"),\n",
    "    AdaBoostClassifier(),\n",
    "    MultinomialNB()]\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', clf)\n",
    "                        ])\n",
    "    pipeline.fit(corpus_train, y_train)\n",
    "    # evalute on the test set\n",
    "    predicted = pipeline.predict(corpus_test)\n",
    "    acc = np.mean(predicted == y_test)\n",
    "    print(\"Model:\", name, \" Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Level 4: the architect\n",
    "Build an end-to-end system with REST APIs for sentiment analysis. Input is a movie review, output is the model prediction. You can stay withing python or use the software stack of your trust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Level 5: the artist\n",
    "Design a beautiful user experience how the predicted labels of the algorithm can be presented to a consumer on a mobile device. Connect it to the REST webservice or just mock up the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
